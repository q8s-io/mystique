[crawler]
load_max_count = 8192                   #高优先级下载url的最大装入数量
download_thread_count =64               #下载线程的个数
log_file = ../config/log.ini            #log配置文件

data_catogory = data
log_catogory = log

crawler_clus = zzzc
crawler_name = crawler_common
dns_type = netcom                       # eg: cmcc, unicom, telcom, netcom, 若不指定或指定有误，蜘蛛自己解析
once_pull_count = 128

[download]                              #下载模块配置参数
download_res_flag = 0
max_redirect_count = 5                  #所处理的重定向的最大次数
download_timeout = 120                  #下载超时时间
connect_timeout = 60                    #连接超时时间
download_time_display = 15              #当下载时间超过该秒数时，打印url信息
meta_jump_max_second = 10               #当需要睡眠的时间超过该值时，不再下载meta跳转
ignore_meta_jump_size = 4000            #当下载的页面长度超过该值时,不再下载meta跳转
ignore_frame_size = 4000                #html的长度超过该值，忽略frame
download_max_length = 1048576           #下载最大长度, 超过此值后截断 注意不支持表达式  不能写1024*1024 !!!!
download_interface =                    #下载时指定出口ip
auto_interface = 1                      #下载时是否自动获取机器外网ip作为出口ip
user_agent = User-Agent:Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0); 360Spider
host_down_url = http://182.118.31.213:8080/downloader.php?url=
spider_name = 360Spider

[clean_thread]                                          #清理线程配置参数
clean_time = 1                                          #多长时间清理一次，以小时为单位 !!!
error_clean_time = 48                                   #待绑定的domain-url list里, 如果某个url的等待时间超过该数值，则清除整个domain记录! Note: 目前是取domain里最老的节点  以小时为单位!
parse_clean_time = 3

[vali]
vali_server = dic32.se.zzbc
vali_port = 12306
vali_permit = 1                                         # 默认匹配规则 1：robots允许 0：robots封禁

[sinan]
sinan_server = 10.115.92.104
sinan_port = 12506

[dns]
dns_server = 10.119.87.90;
dns_port = 53

[ServerList]
server_addrs=10.173.29.244:12015

[TldsDict]
dict = ../config/tlds.dict
